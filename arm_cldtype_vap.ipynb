{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python notebook to illustrate capabilities of ARM JupterLab + [HPC platform](https://www.arm.gov/capabilities/computing-resources).**\n",
    "1. Use ARM Live Web Service for data download\n",
    "2. Generate ARM cloud classification [(CLDTYPE) VAP](https://www.arm.gov/capabilities/vaps/cldtype) product\n",
    "\n",
    "This notebook is not a complete implementation of the [CLDTYPE VAP](https://www.arm.gov/capabilities/vaps/cldtype), rather a simplfied implementation for purpose of demonstration.\n",
    "\n",
    "Author: *Jitendra (Jitu) Kumar, Oak Ridge National Laboratory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ARM Live credentials here\n",
    "user = \"User.Name\"\n",
    "credential = \"add your web key here\"\n",
    "\n",
    "# Get your credentials at https://adc.arm.gov/armlive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import logging\n",
    "import csv\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine for ARM Live Web Service\n",
    "from urllib import request\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def get_files(user: str, credential: str, datastream: str, start: str, end: str, output=None):\n",
    "\n",
    "    # combine user and credential\n",
    "    username = \"{}:{}\".format(user, credential)\n",
    "\n",
    "    # start and end strings for query_url are constructed\n",
    "    if start: start = \"&start={}\".format(start)\n",
    "    if end: end = \"&end={}\".format(end)\n",
    "\n",
    "    # build link, user = username:credentials, datastream can be partial, start and end in form YYYY-MM-DD\n",
    "    query_url = 'https://adc.arm.gov/armlive/livedata/query?user={0}&ds={1}{2}{3}&wt=json'\\\n",
    "        .format(username, datastream, start, end)\n",
    "    #print(\"query url = {}\".format(query_url))\n",
    "\n",
    "    # get url response, read the body of the message, and decode from bytes type to utf-8 string\n",
    "    response_body = request.urlopen(query_url).read().decode(\"utf-8\")\n",
    "    # if the response is an html doc, then there was an error with the user\n",
    "    if response_body[1:14] == \"!DOCTYPE html\":\n",
    "        print(\"Error with user. Check username or token.\")\n",
    "        exit(1)\n",
    "\n",
    "    # parse into json object\n",
    "    response_body_json = json.loads(response_body)\n",
    "    #print(\"response body:\\n{0}\\n\".format(json.dumps(response_body_json, indent=True)))\n",
    "\n",
    "    # construct output directory\n",
    "    if output:\n",
    "        # output files to directory specified\n",
    "        output_dir = os.path.join(output)\n",
    "    else:\n",
    "        # if no folder given, add datastream folder to current working dir to prevent file mix-up\n",
    "        output_dir = os.path.join(os.getcwd(), datastream)\n",
    "\n",
    "    # response is successful and files were returned\n",
    "    if response_body_json[\"status\"] == \"success\" and len(response_body_json[\"files\"]) > 0:\n",
    "        for f in response_body_json['files']:\n",
    "            print(\"[DOWNLOADING] {}\".format(f))\n",
    "            # construct link to web service saveData function\n",
    "            save_data_url = \"https://adc.arm.gov/armlive/livedata/saveData?user={0}&file={1}\"\\\n",
    "                .format(username, f)\n",
    "            #print(\"downloading file: {0}\\n\\tusing link: {1}\".format(f, save_data_url))\n",
    "\n",
    "            output_file = os.path.join(output_dir, f)\n",
    "            # make directory if it doesn't exist\n",
    "            if not os.path.isdir(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            # create file and write bytes to file\n",
    "            with open(output_file, 'wb') as open_bytes_file:\n",
    "                open_bytes_file.write(request.urlopen(save_data_url).read())\n",
    "            #print(\"file saved to --> {}\\n\".format(output_file))\n",
    "    else:\n",
    "        print(\"No files returned or url status error.\\nCheck datastream name, start, and end date.\")\n",
    "    print \"DOWNLOAD COMPLETE\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasets required for CLDTYPE VAP\n",
    "# Download arscl data\n",
    "ds = \"nsaarscl1clothC1.c1\"\n",
    "start = \"2010-03-01\"\n",
    "end = \"2010-03-05\"\n",
    "out=\"live_download_dir\"\n",
    "\n",
    "get_files(user, credential, ds, start, end, out)\n",
    "\n",
    "# Download met data\n",
    "ds = \"nsametC1.b1\"\n",
    "start = \"2010-03-01\"\n",
    "end = \"2010-03-05\"\n",
    "out = \"live_download_dir\"\n",
    "get_files(user, credential, ds, start, end, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routines for cloud classification\n",
    "class create_time_series:\n",
    "    '''\n",
    "    Class to load staged data from ARM database, to parse and analyze it\n",
    "    and save the results as NetCDF file.\n",
    "\n",
    "    Args:\n",
    "        None.\n",
    "\n",
    "    Returns:\n",
    "        None. Saves NetCDF data file.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        '''Initializer for create_time_series class\n",
    "\n",
    "        Args:\n",
    "            filename (str): name for NetCDF output file\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        '''\n",
    "\n",
    "        # initialize self object\n",
    "        self.filename = filename\n",
    "        print(\"Working on file: %s\"%(self.filename))\n",
    "        \n",
    "\n",
    "\n",
    "    def rain_check(self, rain_rate):\n",
    "        '''\n",
    "        Checks the rate of rainfall to see if instrument\n",
    "        measurements are accurate\n",
    "\n",
    "        Args:\n",
    "            rain_rate (float): rainfall rate (mm/hr)\n",
    "\n",
    "        Returns:\n",
    "            rain_bool (bool): boolean determining if instrument\n",
    "                              measurements are accurate\n",
    "        '''\n",
    "\n",
    "        # check to see if it is rainging to much\n",
    "        if rain_rate >= 1:\n",
    "            rain_bool = True\n",
    "            return rain_bool\n",
    "\n",
    "        # else, it is not raining too much\n",
    "        elif rain_rate < 1:\n",
    "            rain_bool = False\n",
    "            return rain_bool\n",
    "\n",
    "\n",
    "    def load_cloud_data(self, file):\n",
    "        '''\n",
    "        Loads data about clouds from NetCDF file\n",
    "\n",
    "        Args:\n",
    "            file (str): file name for data to load\n",
    "\n",
    "        Returns:\n",
    "            time_array (np array): time array of interest\n",
    "            cloud_base_data (np array): raw cloud base measurements\n",
    "            cloud_top_data (np array): raw cloud top measurements\n",
    "        '''\n",
    "\n",
    "        # load cloud data\n",
    "        rootgrp = Dataset(file, \"r\", format=\"NETCDF4\")\n",
    "\n",
    "        # extract cloud data\n",
    "        time_array = rootgrp.variables['time_offset'][:]\n",
    "        cloud_base_data = rootgrp.variables['CloudLayerBottomHeightMplZwang'][:]\n",
    "        cloud_top_data = rootgrp.variables['CloudLayerTopHeightMplZwang'][:]\n",
    "\n",
    "        # create dataset, start writing NetCDF file\n",
    "        rg = Dataset(self.filename, \"w\", format=\"NETCDF4\")\n",
    "\n",
    "        # create base_time\n",
    "        base_time = rg.createDimension('base_time', 1)\n",
    "        bt = rg.createVariable('base_time', 'i4', ('base_time',))\n",
    "        bt[0] = rootgrp.variables['base_time'][:]\n",
    "        bt.units = 'seconds since 1970-1-1 0:00:00 0:00'\n",
    "        bt.long_name = 'Base time in Epoch'\n",
    "        bt.ancillary_variables = 'time_offset'\n",
    "\n",
    "        # create height variable\n",
    "        height = rg.createDimension('height', len(rootgrp['Heights']))\n",
    "        h = rg.createVariable('height', 'f8', ('height',))\n",
    "        h[:] = rootgrp['Heights'][:]\n",
    "        h.units = 'm'\n",
    "        h.long_name = 'Height above ground level'\n",
    "        h.standard_name = 'height'\n",
    "\n",
    "        rg.close()\n",
    "\n",
    "        # close cloud data set\n",
    "        rootgrp.close()\n",
    "\n",
    "        return time_array, cloud_base_data, cloud_top_data\n",
    "\n",
    "    def load_rain_data(self, rain_file):\n",
    "        '''\n",
    "        Loads data from NetCDF file\n",
    "\n",
    "        Args:\n",
    "            rain_file (str): file name for rain data to load\n",
    "\n",
    "        Returns:\n",
    "            rain_array (np array): rainfall measurements (mm/hr)\n",
    "        '''\n",
    "\n",
    "        # load data\n",
    "        rootgrp = Dataset(rain_file, 'r', format='NETCDF4')\n",
    "\n",
    "        # extract data\n",
    "        rain_array = rootgrp.variables['pws_precip_rate_mean_1min'][:]\n",
    "\n",
    "        rootgrp.close()\n",
    "\n",
    "        return rain_array\n",
    "\n",
    "    def classify_cloud(self, cld_bot, cld_top, cld_thick, rn_rate):\n",
    "        '''\n",
    "        Classify the type of cloud given.\n",
    "\n",
    "        Args:\n",
    "            cld_bot (float): Height of the bottom of a cloud\n",
    "            cld_top (float): Height of the top of a cloud\n",
    "            cld_thick (float): Thickness of a cloud\n",
    "            rn_rate (float): rainfall rate during measurement\n",
    "\n",
    "        Returns:\n",
    "            cld_type (int): cloud type represented by an integer\n",
    "        '''\n",
    "\n",
    "        # check for no clouds\n",
    "        if (cld_bot == 0 and cld_top == 0):\n",
    "            cld_type = -9999\n",
    "\n",
    "        elif self.rain_check(rn_rate):\n",
    "            cld_type = -9999\n",
    "\n",
    "        # Identify low level clouds\n",
    "        elif cld_bot < 4000:\n",
    "\n",
    "            # Low clouds\n",
    "            if cld_top < 4000:\n",
    "                cld_type = 1\n",
    "\n",
    "            # Congestus clouds\n",
    "            elif 4000 <= cld_top <= 8000:\n",
    "                cld_type = 2\n",
    "\n",
    "            # Deep convection clouds\n",
    "            elif cld_top > 8000:\n",
    "                cld_type = 3\n",
    "\n",
    "        # Medium height clouds\n",
    "        elif 4000 <= cld_bot <= 8000:\n",
    "\n",
    "            # Altocumulus clouds\n",
    "            if ((4000 <= cld_top <= 8000) and cld_thick < 1500):\n",
    "                cld_type = 4\n",
    "\n",
    "            # Altostratus clouds\n",
    "            elif ((4000 <= cld_top <= 8000) and cld_thick >= 1500):\n",
    "                cld_type = 5\n",
    "\n",
    "            # Cirrostratus/Anvil clouds\n",
    "            elif (cld_top > 8000):\n",
    "                cld_type = 6\n",
    "\n",
    "        # Cirrus clouds\n",
    "        elif (cld_bot > 8000):\n",
    "            cld_type = 7\n",
    "\n",
    "        return cld_type\n",
    "\n",
    "    def analyze_data(self, cloud_base_data, cloud_top_data, rain_rate, time_array):\n",
    "        '''\n",
    "        Analyzes cloud data and determines what types of cloud there are\n",
    "\n",
    "        Args:\n",
    "            cloud_base_data (np array): raw cloud base height\n",
    "            cloud_top_data (np array): raw cloud top height\n",
    "            rain_rate (float): rainfall rate (mm/hr)\n",
    "            time_array (np array): time array of interest\n",
    "\n",
    "        Returns:\n",
    "            cloud_layer_array (np array): classification of cloud layers\n",
    "            time_bounds_array (np array): time bounds array\n",
    "            new_time_array (np array): new array for time, after averaging over 1 minute\n",
    "        '''\n",
    "\n",
    "        # create empty array cloud layer classification\n",
    "        cloud_layer_array = np.zeros((int((len(cloud_base_data)/6)), 10))\n",
    "\n",
    "        # create time bounds array\n",
    "        time_bounds_array = np.zeros((int((len(cloud_base_data)/6)), 2))\n",
    "        new_time_array = np.zeros((int((len(cloud_base_data)/6))))\n",
    "\n",
    "        # loop over all data\n",
    "        for i in range(0, 1440):\n",
    "\n",
    "            # update time bounds\n",
    "            time_bounds_array[i, 0] = i\n",
    "            time_bounds_array[i, 1] = (i+1)\n",
    "\n",
    "            # update new time array in minutes\n",
    "            new_time_array[i] = i\n",
    "\n",
    "            for j in range(0, 10):\n",
    "\n",
    "                cld_type_list = []\n",
    "\n",
    "                # iterate over 1 minute interval to find most occuring\n",
    "                # cloud type\n",
    "                for k in range(0,6):\n",
    "\n",
    "                    # get cloud top and bottom and thickness\n",
    "                    cld_bot = cloud_base_data[(i*6+k), j]\n",
    "                    cld_top = cloud_top_data[(i*6+k), j]\n",
    "                    cld_thick = cld_top - cld_bot\n",
    "\n",
    "                    # get rain rate\n",
    "                    rn_rate = rain_rate[i]\n",
    "\n",
    "                    # clasifyl cloud\n",
    "                    cld_type = self.classify_cloud(cld_bot, cld_top, cld_thick, rn_rate)\n",
    "\n",
    "                    # add cloud type to list\n",
    "                    cld_type_list.append(cld_type)\n",
    "\n",
    "                # find most frequently occuring cloud type at cloud level\n",
    "                cld_type = max(set(cld_type_list), key=cld_type_list.count)\n",
    "\n",
    "                # update cloud type in array\n",
    "                cloud_layer_array[i, j] = cld_type\n",
    "\n",
    "        return cloud_layer_array, time_bounds_array, new_time_array\n",
    "\n",
    "    def save_cloud_data(self, new_time_array, time_bounds_array, cloud_layer_array, cloud_base_data):\n",
    "        '''\n",
    "        Creates NetCDF file from time and cloud layer data\n",
    "\n",
    "        Args:\n",
    "            new_time_array (np array): time array of interest\n",
    "            time_bounds_array (np array): time bounds array\n",
    "            cloud_layers (np array): classification of cloud layers\n",
    "            cloud_base_data (np array): raw cloud base height\n",
    "\n",
    "        Returns:\n",
    "            Mone. Saves NetCDF file.\n",
    "        '''\n",
    "\n",
    "        # create dataset, start writing NetCDF file\n",
    "        rootgrp = Dataset(self.filename, \"a\", format=\"NETCDF4\")\n",
    "\n",
    "        # create dimensions for NetCDF file\n",
    "        time_offset = rootgrp.createDimension('time_offset', len(new_time_array))\n",
    "        time = rootgrp.createDimension('time', len(new_time_array))\n",
    "        cloudtype = rootgrp.createDimension('cloudtype', len(cloud_layer_array))\n",
    "        layer = rootgrp.createDimension('layer', 10)\n",
    "\n",
    "        # create time_offset variable\n",
    "        t = rootgrp.createVariable('time_offset', 'f8', ('time_offset',))\n",
    "        t.long_name = 'Time Offset from base_time'\n",
    "        t.units = 'minutes'\n",
    "        t.ancillary_variables = 'base_time'\n",
    "        t[:] = new_time_array[:]\n",
    "\n",
    "        # create time variable\n",
    "        t_m = rootgrp.createVariable('time', 'f8', ('time',))\n",
    "        t_m.long_name = 'Time Offset from midnight'\n",
    "        t_m.units = 'minutes'\n",
    "        t_m.ancillary_variables = 'time_bounds'\n",
    "        t_m[:] = new_time_array[:]\n",
    "\n",
    "        # create time bounds variable\n",
    "        time_bounds = rootgrp.createDimension('time_bounds', 2)\n",
    "        tb = rootgrp.createVariable('time_bounds', 'f8', ('time_offset', 'time_bounds'))\n",
    "        tb[:] = time_bounds_array[:]\n",
    "        tb.long_name = 'Time cell bounds'\n",
    "        tb.bound_offsets = -30, 30\n",
    "\n",
    "        # create layer variable\n",
    "        l = rootgrp.createVariable('layer', 'i4', ('layer'))\n",
    "        l.long_name = 'Cloud layer number'\n",
    "        l.units = 'unitless'\n",
    "        l[:] = np.arange(1,11,1)\n",
    "\n",
    "        # create cloud type variable\n",
    "        cld_layer = rootgrp.createVariable('cloudtype', 'i4', ('cloudtype', 'layer'))\n",
    "        cld_layer[:] = cloud_layer_array[:]\n",
    "        cld_layer.long_name = 'Cloud type'\n",
    "        cld_layer.units = 'unitless'\n",
    "        cld_layer.ancillary_variables = 'qc_cloudtype'\n",
    "        cld_layer.missing_value = -9999\n",
    "        cld_layer.flag_values = 1, 2, 3, 4, 5, 6, 7\n",
    "        cld_layer.flag_meanings = 'low_cloud', 'congestus', 'deep_convection', 'altocumulus', 'altostratus', 'cirrostratus/anvil', 'cirrus'\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def cloud_analysis(self, file, rain_file):\n",
    "        '''Main algrithm, loads cloud data and analyzes it. Returns data as a NetCDF file.\n",
    "\n",
    "        Args:\n",
    "            file (str): file name for data to load\n",
    "            rain_file (str): file name for rain data to load\n",
    "\n",
    "        Returns:\n",
    "            None. Saves data to NetCDF file.\n",
    "        '''\n",
    "\n",
    "        # split to allow it to load from DB\n",
    "\n",
    "        # load cloud data\n",
    "        time_array, cloud_base_data, cloud_top_data = self.load_cloud_data(file)\n",
    "\n",
    "        # load rain data\n",
    "        rain_rate = self.load_rain_data(rain_file)\n",
    "\n",
    "\n",
    "        # separate time series creation and netCDF creation\n",
    "        # use in memory data structure for manipulation of data\n",
    "        # make data file (netCDF, CSV,...) afterwards\n",
    "\n",
    "        # analyze cloud data\n",
    "        cloud_layer_array, time_bounds_array, new_time_array = self.analyze_data(cloud_base_data, cloud_top_data, rain_rate, time_array)\n",
    "\n",
    "        # save cloud data\n",
    "        self.save_cloud_data(new_time_array, time_bounds_array, cloud_layer_array, cloud_base_data)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CLDTYPE VAP for downloaded data\n",
    "directory='live_download_dir'\n",
    "\n",
    "file_list = glob.glob(directory + \"/*arscl*.cdf\")\n",
    "num_files = len(file_list)\n",
    "\n",
    "# run cloud classification on each file\n",
    "for f in range(num_files):\n",
    "    filename=file_list[f]\n",
    "    split_file = filename.split('/')\n",
    "    arscl_file = split_file[-1]\n",
    "    met_file = directory + '/' + arscl_file[0:3] + 'metC1.b1' + arscl_file[-20:]\n",
    "    cc_file = directory + '/nsa_C1_cloud_classification' + arscl_file[-20:-11] + '.cdf'\n",
    "\n",
    "    ts = create_time_series(filename=cc_file)\n",
    "    ts.cloud_analysis(file=filename, rain_file=met_file)\n",
    "print \"COMPLETE: Cloud Classification VAP\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dask-env]",
   "language": "python",
   "name": "conda-env-dask-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
